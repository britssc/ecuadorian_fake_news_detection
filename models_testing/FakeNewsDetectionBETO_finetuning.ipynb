{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/britssc/ecuadorian_fake_news_detection/blob/main/FakeNewsDetectionBETO_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detection: BETO Model Fine-Tuning Process\n",
        "\n",
        "Ecuador's 2025 Election Dataset (623 News)"
      ],
      "metadata": {
        "id": "h-BtdM7vi2bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías\n",
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "NC5YIaeHxVha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Usar el GPU si está disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "H2F6qNy0xXyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yA65nK5-xcvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer el directorio de trabajo\n",
        "#%cd /content/drive/MyDrive/University/8th\\ Semester/5.\\ Scientific\\ Communication\\ Workshop/X_Fake_News_Detection"
      ],
      "metadata": {
        "id": "bsXRLbM8xfuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "\n",
        "#Note: Upload the file from the repository: https://github.com/britssc/ecuadorian_fake_news_detection\n",
        "df = pd.read_csv(\"clean_data.csv\")\n",
        "\n",
        "df = df.dropna()\n",
        "df['real'] = df['real'].astype(int)\n",
        "\n",
        "#df"
      ],
      "metadata": {
        "id": "9ZyHwh-HxvRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chequear si nuestros datos están bien balanceados\n",
        "label_size = [df['real'].sum(), len(df['real']) - df['real'].sum()]\n",
        "plt.pie(label_size, explode = [0.1,0.1], colors = ['firebrick','navy'], startangle = 90, shadow = True, labels = ['0', '1'], autopct = '%1.1f%%')"
      ],
      "metadata": {
        "id": "Gqb--t81LLac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conjunto Prueba-Validación-Prueba dividido en un radio 70:15:15\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df['text'].tolist(), df['real'].tolist(), stratify = df['real'], test_size = 0.3, random_state = 42)\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, stratify = temp_labels, test_size = 0.5, random_state = 42)"
      ],
      "metadata": {
        "id": "7Po7BKfayDZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegir modelo\n",
        "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "DMIK-33iyGLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización\n",
        "def tokenize(texts, max_len = 400):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding = 'max_length',\n",
        "        truncation = True,\n",
        "        max_length = max_len,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize(train_texts)\n",
        "val_encodings = tokenize(val_texts)\n",
        "test_encodings = tokenize(test_texts)"
      ],
      "metadata": {
        "id": "CB5BQqPhyN3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear DataLoaders\n",
        "def create_dataset(encodings, labels):\n",
        "    return TensorDataset(\n",
        "        encodings['input_ids'],\n",
        "        encodings['attention_mask'],\n",
        "        torch.tensor(labels)\n",
        "    )\n",
        "\n",
        "train_dataset = create_dataset(train_encodings, train_labels)\n",
        "val_dataset = create_dataset(val_encodings, val_labels)\n",
        "test_dataset = create_dataset(test_encodings, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size = 16)\n",
        "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size = 16)\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size = 16)"
      ],
      "metadata": {
        "id": "G5dDPF7pQyJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir arquitectura\n",
        "class BETOClassifier(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(BETOClassifier, self).__init__()\n",
        "        self.beto = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.beto.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.beto(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        return self.classifier(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BETOClassifier(model_name).to(device)"
      ],
      "metadata": {
        "id": "F5krLHsmQ9uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizador y criterio\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "UbXj8w10yiOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento y evaluación\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim = 1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    return predictions, true_labels"
      ],
      "metadata": {
        "id": "yMEKp12nykY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento\n",
        "for epoch in range(3):\n",
        "    loss = train_epoch(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "hKFXpRDuynCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación final en test\n",
        "preds, trues = evaluate(model, test_loader)\n",
        "#print(classification_report(trues, preds, digits = 4))"
      ],
      "metadata": {
        "id": "a1Ghp-m0y5wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardando los resultados"
      ],
      "metadata": {
        "id": "h-Hx2XXlmmkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_metrics(trues, preds, model_name=\"model\", csv_path=\"beto_metrics.csv\"):\n",
        "    report = classification_report(trues, preds, digits=4, output_dict=True)\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": report[\"accuracy\"],\n",
        "        \"Weighted Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"Weighted Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"Macro F1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"Class 0 F1\": report[\"0\"][\"f1-score\"],\n",
        "        \"Class 1 F1\": report[\"1\"][\"f1-score\"]\n",
        "    }\n",
        "    df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Métricas guardadas en '{csv_path}'\")"
      ],
      "metadata": {
        "id": "0hkOulELmoey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_metrics(trues, preds, model_name=\"beto\")\n"
      ],
      "metadata": {
        "id": "XZejCJNDp-7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
