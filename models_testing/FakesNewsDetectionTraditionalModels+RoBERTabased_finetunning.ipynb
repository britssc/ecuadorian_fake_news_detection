{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/britssc/ecuadorian_fake_news_detection/blob/main/models_testing/FakesNewsDetectionTraditionalModels%2BRoBERTabased_finetunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detection: Traditional ML models + RoBERTa finetuning\n",
        "\n",
        "Ecuador's 2025 Election Dataset (623 News)"
      ],
      "metadata": {
        "id": "9Etn2v2lHY9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52BOPUvDvBHf"
      },
      "outputs": [],
      "source": [
        "!pip install SciencePlots\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.style.use('seaborn-v0_8-white')\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet Analysis"
      ],
      "metadata": {
        "id": "hy3hSiYQHse9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ3NxUqAvBHi"
      },
      "source": [
        "## 1. Reading the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_-AHkTFvBHj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scienceplots\n",
        "plt.style.use(['science', 'notebook', 'no-latex'])\n",
        "\n",
        "data = pd.read_excel('Datos.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYHUqNo5vBHl"
      },
      "source": [
        "## 2. Visualization of the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTCavO5RvBHo"
      },
      "outputs": [],
      "source": [
        "# Tweets of each type found in the training dataset.\n",
        "data['real'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnJJeW2KvBHp"
      },
      "outputs": [],
      "source": [
        "\n",
        "data['real'].hist()\n",
        "plt.ylabel(\"# tweets\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGJ4tBahvBHp"
      },
      "source": [
        "Always perform an exploratory analysis of the data distribution to determine the best way to solve the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcG57p51vBHq"
      },
      "source": [
        "### Number of words per Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2cTybkTvBHr"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "# We calculate the number of words\n",
        "tweet_len_0 = data[data['real'] == 0]['text'].str.split().map(lambda x: len(x))\n",
        "tweet_len_1 = data[data['real'] == 1]['text'].str.split().map(lambda x: len(x))\n",
        "\n",
        "ax1.hist(tweet_len_0, color='red')\n",
        "ax1.set_title('fake tweets')\n",
        "ax1.set_xlim([0,1100])\n",
        "\n",
        "ax2.hist(tweet_len_1, color='green')\n",
        "ax2.set_title('real tweets')\n",
        "ax2.set_xlim([0,1100])\n",
        "\n",
        "fig.suptitle('Number of words per tweet')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79bccWrvBHr"
      },
      "source": [
        "### Number of unique words per Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_kMrIJxvBHs"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "# We calculate the number of words\n",
        "tweet_len_0 = data[data['real'] == 0]['text'].str.split().map(lambda x: len(set(x)))\n",
        "tweet_len_1 = data[data['real'] == 1]['text'].str.split().map(lambda x: len(set(x)))\n",
        "\n",
        "ax1.hist(tweet_len_0, color='red')\n",
        "ax1.set_title('fake news')\n",
        "ax1.set_xlim([0,500])\n",
        "\n",
        "ax2.hist(tweet_len_1, color='green')\n",
        "ax2.set_title('real news')\n",
        "ax2.set_xlim([0,500])\n",
        "\n",
        "fig.suptitle('Number of words per tweet')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekO385NsvBHt"
      },
      "source": [
        "### Average word length per Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y6bsa5-vBHt"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "# We calculate the number of words\n",
        "tweet_len_0 = data[data['real'] == 0]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))\n",
        "tweet_len_1 = data[data['real'] == 1]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))\n",
        "\n",
        "ax1.hist(tweet_len_0, color='red')\n",
        "ax1.set_title('fake news')\n",
        "ax1.set_xlim([0,18])\n",
        "\n",
        "ax2.hist(tweet_len_1, color='green')\n",
        "ax2.set_title('real news')\n",
        "ax2.set_xlim([0,18])\n",
        "\n",
        "fig.suptitle('Average word length per Tweet')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7lEz_FEvBHu"
      },
      "source": [
        "### Number of characters per tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe5sjswGvBHv"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "# Calculamos el n√∫mero de caracteres por tweet\n",
        "tweet_len_0 = data[data['real'] == 0]['text'].str.len()\n",
        "tweet_len_1 = data[data['real'] == 1]['text'].str.len()\n",
        "\n",
        "ax1.hist(tweet_len_0, color='red')\n",
        "ax1.set_title('fake news')\n",
        "ax1.set_xlim([0,6000])\n",
        "\n",
        "ax2.hist(tweet_len_1, color='green')\n",
        "ax2.set_title('real news')\n",
        "ax2.set_xlim([0,6000])\n",
        "\n",
        "fig.suptitle('Number of characters per tweet')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk4xnAvZvBHv"
      },
      "source": [
        "We could continue to calculate input characteristics such as the following:\n",
        "* Number of end words per Tweet\n",
        "* Number of urls per Tweet\n",
        "* Average number of characters per Tweet\n",
        "* Number of characters per Tweet\n",
        "* Number of punctuation marks per Tweet\n",
        "* Number of hashtags per Tweet\n",
        "* Number of @'s per tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xBy17LwvBHw"
      },
      "source": [
        "### Most used stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLAhLAWnvBHw"
      },
      "source": [
        "These words do not have a meaning by themselves, but modify or accompany others, this group usually consists of articles, pronouns, prepositions, adverbs and even some verbs.\n",
        "\n",
        "In natural language data processing they are filtered before or after the process itself, they are not considered because they have no meaning, in the case of search engines such as Google they are not considered at the time of positioning, but they are considered at the time of displaying search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRJWskQ1vBHx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKQTJCq0vBHy"
      },
      "outputs": [],
      "source": [
        "def plot_stopwords(label):\n",
        "    tweets_stopwords = {}\n",
        "    for words in data[data['real'] == label]['text'].str.split():\n",
        "        sw = list(set(words).intersection(stopwords.words('spanish')))\n",
        "        for w in sw:\n",
        "            if w in tweets_stopwords.keys():\n",
        "                tweets_stopwords[w] += 1\n",
        "            else:\n",
        "                tweets_stopwords[w] = 1\n",
        "\n",
        "    top = sorted(tweets_stopwords.items(), key=lambda x:x[1],reverse=True)[:10]\n",
        "    plt.bar(*zip(*top))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLKqVFJ9vBHy"
      },
      "outputs": [],
      "source": [
        "#Fake news.\n",
        "from nltk.corpus import stopwords\n",
        "plot_stopwords(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKXyZW8bvBHz"
      },
      "outputs": [],
      "source": [
        "# real words\n",
        "plot_stopwords(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZqv6MoFvBHz"
      },
      "source": [
        "### Punctuation analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh9OJS9avBHz"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def plot_punctuation(label):\n",
        "    tweets_stopwords = {}\n",
        "    for words in data[data['real'] == label]['text'].str.split():\n",
        "        sw = list(set(words).intersection(string.punctuation))\n",
        "        for w in sw:\n",
        "            if w in tweets_stopwords.keys():\n",
        "                tweets_stopwords[w] += 1\n",
        "            else:\n",
        "                tweets_stopwords[w] = 1\n",
        "\n",
        "    top = sorted(tweets_stopwords.items(), key=lambda x:x[1],reverse=True)[:20]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(*zip(*top))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hpz8NOMvBH0"
      },
      "outputs": [],
      "source": [
        "#fake news\n",
        "plot_punctuation(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDJfTl6-vBH0"
      },
      "outputs": [],
      "source": [
        "# real news\n",
        "plot_punctuation(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iuckEB0vBH1"
      },
      "source": [
        "### Ngram analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1M3u4tGvBH1"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(2, 2))\n",
        "sum_words = cv.fit_transform(data['text']).sum(axis=0)\n",
        "\n",
        "# We calculate\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtVn6MPkvBH1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.barh(*zip(*words_freq))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y6AK6wsvBH2"
      },
      "source": [
        "## 3. Data set cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W3HaBKSvBH2"
      },
      "outputs": [],
      "source": [
        "import re # to define regular expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc69xkjgvBH3"
      },
      "outputs": [],
      "source": [
        "def remove_url(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjU0fPQUvBH3"
      },
      "outputs": [],
      "source": [
        "remove_url(\"Esto es una prueba: http://localhost:8888/notebooks/Desktop/Workspace/Deep%20Neural%20Networks%20Course/11.%20Consideraciones%20de%20un%20proyecto%20de%20Deep%20Learning/code/Disaster%20Tweets.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uts_TYUuvBIA"
      },
      "outputs": [],
      "source": [
        "from html.parser import HTMLParser\n",
        "\n",
        "class HTMLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs = True\n",
        "        self.fed = []\n",
        "\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)\n",
        "\n",
        "def remove_html(text):\n",
        "    s = HTMLStripper()\n",
        "    s.feed(text)\n",
        "    return s.get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybUwVpTvvBIB"
      },
      "outputs": [],
      "source": [
        "remove_html('<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcrV1nqCvBIB"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o0jzxKnvBIC"
      },
      "outputs": [],
      "source": [
        "remove_emoji(\"Omg another Earthquake üòîüòî\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvHoVRrvBID"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_jnuLisvBID"
      },
      "outputs": [],
      "source": [
        "remove_punctuation(\"hello # how are you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjIV0ZtpvBIE"
      },
      "outputs": [],
      "source": [
        "# We apply our dataset cleaning functions\n",
        "data_prep = data.copy()\n",
        "\n",
        "data_prep['text'] = data['text'].apply(remove_url)\n",
        "data_prep['text'] = data_prep['text'].apply(remove_html)\n",
        "data_prep['text'] = data_prep['text'].apply(remove_emoji)\n",
        "data_prep['text'] = data_prep['text'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRUlYB8CvBIF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(2, 2))\n",
        "sum_words = cv.fit_transform(data_prep['text']).sum(axis=0)\n",
        "\n",
        "# We calculate\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDy6whHcvBIG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.barh(*zip(*words_freq))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPTnqvEvBIG"
      },
      "source": [
        "## 4. Vectorization of the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWRbUvBvBIG"
      },
      "outputs": [],
      "source": [
        "y = data_prep['real']\n",
        "X = data_prep['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3KrECqvvBIJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.30)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyElt1wVvBIH"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer #based in frequencies\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_val = vectorizer.transform(X_val)\n",
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ltW2AQNvBIH"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.toarray()\n",
        "X_val = X_val.toarray()\n",
        "X_test = X_test.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00OKQ2GcvBII"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaEbfVPMg_u_"
      },
      "outputs": [],
      "source": [
        "X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgaWfs0TvBIJ"
      },
      "outputs": [],
      "source": [
        "print(\"Length of training subset: \", len(X_train))\n",
        "print(\"Length of validation subset: \", len(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmorgvU-hqK8"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eEBNbXhvDWu"
      },
      "source": [
        "# Fake News Detection:\n",
        "# Opci√≥n 1: Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea7yIomUvBIK"
      },
      "source": [
        "## 6. Model construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv0txHcjvBIK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "#model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "#model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EOseb79vBIL"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuWeTzsavBIL"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    epochs=10,\n",
        "    batch_size=100,\n",
        "    validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7ucAkD7svDf"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL6RbHYNvBIM"
      },
      "source": [
        "## 7. Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n2krizavBIM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "Y_pred = model.predict(X_test).round(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "Bhe5UBO8O6AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "6OLyDioJIrAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Guardando los resultados\n",
        "def save_metrics(trues, preds, model_name=\"model\", csv_path=\"nn1.csv\"):\n",
        "    report = classification_report(trues, preds, digits=4, output_dict=True)\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": report[\"accuracy\"],\n",
        "        \"Weighted Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"Weighted Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"Macro F1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"Class 0 F1\": report[\"0\"][\"f1-score\"],\n",
        "        \"Class 1 F1\": report[\"1\"][\"f1-score\"]\n",
        "    }\n",
        "    df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"M√©tricas guardadas en '{csv_path}'\")"
      ],
      "metadata": {
        "id": "q18L-UwPOZrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_metrics(Y_test, Y_pred, model_name=\"nn1\")"
      ],
      "metadata": {
        "id": "J1ij7wllOcv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WJfJCZjxA1G"
      },
      "source": [
        "# OPCION 2: REGRESI√ìN LOG√çSTICA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD_PuySbvBIN"
      },
      "outputs": [],
      "source": [
        "# Importaciones adicionales necesarias\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Configuraci√≥n de dispositivo (GPU si est√° disponible)\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnVwX-dXrDPv"
      },
      "outputs": [],
      "source": [
        "# Modelo tradicional de Machine Learning\n",
        "print(\"\\nEntrenando modelo de Regresi√≥n Log√≠stica...\")\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "logreg_model.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluaci√≥n\n",
        "#logreg_val_pred = logreg_model.predict(X_val)\n",
        "#print(\"\\nResultados de Regresi√≥n Log√≠stica (Validation Set):\")\n",
        "#print(classification_report(Y_val, logreg_val_pred))\n",
        "\n",
        "logreg_test_pred = logreg_model.predict(X_test)\n",
        "print(\"\\nResultados de Regresi√≥n Log√≠stica (Test Set):\")\n",
        "print(classification_report(Y_test, logreg_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "D0mxnORlPJRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Guardando los resultados\n",
        "def save_metrics(trues, preds, model_name=\"model\", csv_path=\"reg_log.csv\"):\n",
        "    report = classification_report(trues, preds, digits=4, output_dict=True)\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": report[\"accuracy\"],\n",
        "        \"Weighted Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"Weighted Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"Macro F1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"Class 0 F1\": report[\"0\"][\"f1-score\"],\n",
        "        \"Class 1 F1\": report[\"1\"][\"f1-score\"]\n",
        "    }\n",
        "    df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"M√©tricas guardadas en '{csv_path}'\")"
      ],
      "metadata": {
        "id": "bdoNu7UPPJRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_metrics(Y_test, logreg_test_pred, model_name=\"regression\")"
      ],
      "metadata": {
        "id": "QM0aswx_PJRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTUnsueNJNCR"
      },
      "source": [
        "# OPCI√ìN 3: RoBERTa based model\n",
        "\n",
        "Website link: https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne-sqac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7vEuVHVy5z-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuraci√≥n del dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Divisi√≥n de datos\n",
        "X_train_Maria, X_temp_Maria, y_train_Maria, y_temp_Maria = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val_Maria, X_test_Maria, y_val_Maria, y_test_Maria = train_test_split(X_temp_Maria, y_temp_Maria, test_size=0.5, random_state=42)\n",
        "\n",
        "# Tokenizador\n",
        "tokenizer_Maria = RobertaTokenizer.from_pretrained(\"BSC-TeMU/roberta-base-bne\")\n",
        "\n",
        "def tokenize_function_Maria(examples):\n",
        "    return tokenizer_Maria(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Creaci√≥n de datasets\n",
        "train_dataset_Maria = Dataset.from_dict({\"text\": X_train_Maria, \"label\": y_train_Maria})\n",
        "val_dataset_Maria = Dataset.from_dict({\"text\": X_val_Maria, \"label\": y_val_Maria})\n",
        "test_dataset_Maria = Dataset.from_dict({\"text\": X_test_Maria, \"label\": y_test_Maria})\n",
        "\n",
        "# Tokenizaci√≥n\n",
        "train_dataset_Maria = train_dataset_Maria.map(tokenize_function_Maria, batched=True)\n",
        "val_dataset_Maria = val_dataset_Maria.map(tokenize_function_Maria, batched=True)\n",
        "test_dataset_Maria = test_dataset_Maria.map(tokenize_function_Maria, batched=True)\n",
        "\n",
        "# Modelo\n",
        "model_maria = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"BSC-TeMU/roberta-base-bne\",\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "# Funci√≥n para calcular m√©tricas\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'f1': f1_score(labels, predictions),\n",
        "        'precision': precision_score(labels, predictions),\n",
        "        'recall': recall_score(labels, predictions),\n",
        "    }\n",
        "\n",
        "# Argumentos de entrenamiento\n",
        "training_args_Maria = TrainingArguments(\n",
        "    output_dir='./results_model_maria',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs_model_maria',\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',  # Ahora esta m√©trica est√° disponible\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer_Maria = Trainer(\n",
        "    model=model_maria,\n",
        "    args=training_args_Maria,\n",
        "    train_dataset=train_dataset_Maria,\n",
        "    eval_dataset=val_dataset_Maria,\n",
        "    compute_metrics=compute_metrics  # ¬°Funci√≥n de m√©tricas a√±adida!\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "print(\"\\nEntrenando modelo_maria...\")\n",
        "trainer_Maria.train()\n",
        "\n",
        "# Evaluaci√≥n\n",
        "print(\"\\nEvaluando en conjunto de test...\")\n",
        "test_predictions_Maria = trainer_Maria.predict(test_dataset_Maria)\n",
        "logits_Maria = test_predictions_Maria.predictions\n",
        "labels_Maria = test_predictions_Maria.label_ids\n",
        "\n",
        "predictions_Maria = np.argmax(logits_Maria, axis=-1)\n",
        "\n",
        "# C√°lculo de m√©tricas\n",
        "accuracy_model_maria = accuracy_score(labels_Maria, predictions_Maria)\n",
        "precision_model_maria = precision_score(labels_Maria, predictions_Maria)\n",
        "recall_model_maria = recall_score(labels_Maria, predictions_Maria)\n",
        "f1_model_maria = f1_score(labels_Maria, predictions_Maria)\n",
        "probs_Maria = torch.softmax(torch.tensor(logits_Maria), dim=1).numpy()\n",
        "auc_model_maria = roc_auc_score(labels_Maria, probs_Maria[:, 1])\n",
        "\n",
        "print(\"\\nResultados finales de model_maria en test:\")\n",
        "print(f\"Accuracy: {accuracy_model_maria:.4f}\")\n",
        "print(f\"Precision: {precision_model_maria:.4f}\")\n",
        "print(f\"Recall: {recall_model_maria:.4f}\")\n",
        "print(f\"F1-score: {f1_model_maria:.4f}\")\n",
        "print(f\"AUC: {auc_model_maria:.4f}\")\n",
        "\n",
        "model_maria.save_pretrained('./modelo_maria_fake_news')\n",
        "tokenizer_Maria.save_pretrained('./modelo_maria_fake_news')\n",
        "\n",
        "print(\"\\nVariables con las m√©tricas:\")\n",
        "print(f\"accuracy_model_maria = {accuracy_model_maria}\")\n",
        "print(f\"precision_model_maria = {precision_model_maria}\")\n",
        "print(f\"recall_model_maria = {recall_model_maria}\")\n",
        "print(f\"f1_model_maria = {f1_model_maria}\")\n",
        "print(f\"auc_model_maria = {auc_model_maria}\")"
      ],
      "metadata": {
        "id": "GNZzRtTbNFjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "Fd-X_2mxPm9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardando los resultados\n",
        "def save_metrics(trues, preds, model_name=\"model\", csv_path=\"RoBeRTa_based.csv\"):\n",
        "    report = classification_report(trues, preds, digits=4, output_dict=True)\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": report[\"accuracy\"],\n",
        "        \"Weighted Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"Weighted Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"Macro F1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"Class 0 F1\": report[\"0\"][\"f1-score\"],\n",
        "        \"Class 1 F1\": report[\"1\"][\"f1-score\"]\n",
        "    }\n",
        "    df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"M√©tricas guardadas en '{csv_path}'\")"
      ],
      "metadata": {
        "id": "q2fPym0xPm9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(labels_Maria, predictions_Maria))"
      ],
      "metadata": {
        "id": "HbEvYakIQ7Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_metrics(labels_Maria, predictions_Maria, model_name=\"RoBERTa_based\")"
      ],
      "metadata": {
        "id": "8Qdu3M8NPm9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Opci√≥n 4: Neural Network 2:"
      ],
      "metadata": {
        "id": "HU_rahcqJTUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D53nhmhrrMLS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Modelo neuronal mejorado\n",
        "print(\"\\nEntrenando modelo de Red Neuronal mejorada...\")\n",
        "improved_nn = models.Sequential()\n",
        "improved_nn.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "improved_nn.add(layers.Dropout(0.5))\n",
        "improved_nn.add(layers.Dense(64, activation='relu'))\n",
        "improved_nn.add(layers.Dropout(0.3))\n",
        "improved_nn.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "improved_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', 'Precision', 'Recall']\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history_improved = improved_nn.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    epochs=30,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluaci√≥n\n",
        "nn_test_pred = (improved_nn.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(\"\\nResultados de Red Neuronal Mejorada (Test Set):\")\n",
        "print(classification_report(Y_test, nn_test_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "Gdy3Py47S4-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardando los resultados\n",
        "def save_metrics(trues, preds, model_name=\"model\", csv_path=\"nn2.csv\"):\n",
        "    report = classification_report(trues, preds, digits=4, output_dict=True)\n",
        "    row = {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": report[\"accuracy\"],\n",
        "        \"Weighted Precision\": report[\"weighted avg\"][\"precision\"],\n",
        "        \"Weighted Recall\": report[\"weighted avg\"][\"recall\"],\n",
        "        \"Macro F1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"Class 0 F1\": report[\"0\"][\"f1-score\"],\n",
        "        \"Class 1 F1\": report[\"1\"][\"f1-score\"]\n",
        "    }\n",
        "    df = pd.DataFrame([row])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"M√©tricas guardadas en '{csv_path}'\")"
      ],
      "metadata": {
        "id": "wF2KJhSVS4-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_metrics(Y_test, nn_test_pred, model_name=\"nn2\")"
      ],
      "metadata": {
        "id": "wy8gxNRIS4-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}